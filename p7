# Install if not already installed
# pip install nltk scikit-learn

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk import pos_tag
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Download required resources
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('wordnet')

# Sample document
document = "Text analytics helps to extract useful insights from unstructured text data."

# 1. Tokenization
tokens = word_tokenize(document)
print("1. Tokenization:\n", tokens)

# # 2. POS Tagging
# pos_tags = pos_tag(tokens)
# print("\n2. POS Tagging:\n", pos_tags)

# 3. Stop Words Removal
stop_words = set(stopwords.words('english'))
filtered = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]
print("\n3. After Stop Words Removal:\n", filtered)

# 4. Stemming
stemmer = PorterStemmer()
stemmed = [stemmer.stem(word) for word in filtered]
print("\n4. Stemming:\n", stemmed)

# 5. Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized = [lemmatizer.lemmatize(word) for word in filtered]
print("\n5. Lemmatization:\n", lemmatized)

# 6. TF-IDF Calculation (for 2 sample documents)
documents = [
    "Text analytics helps extract insights from text.",
    "Analytics tools are used for processing text data."
]

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

print("\n6. TF-IDF Matrix:\n", tfidf_matrix.toarray())
print("\nTF-IDF Feature Names:\n", vectorizer.get_feature_names_out())
